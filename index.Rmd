--- 
title: "How and Why to use Multiple Imputation"
author: "Thomas Lumley"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
keep_tex: yes
output:
  tufte::tufte_handout:
      toc: yes
  bookdown::tufte_html2:
      toc: yes
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
description: "This project was funded by Precision Driven Health"
---

# Executive Summary

Multiple imputation is a valuable technique for reducing bias when training predictive models in health data with missing values, and for increasing applicability when deploying the models in data with missing values.

Multiple imputation works by creating a representative sample of plausible values that the missing data could have had, and then averaging the analysis over that sample.  Using randomly-sampled representative values reduces bias, and using a sample of multiple plausible values for each missing value allows for valid estimates of uncertainty. 

In large datasets, the classical methods of multiple imputation are computationally challenging. One goal of this project was to investigate multiple imputation using modern machine learning methods: random forests, and deep neural networks. These approaches are computationally feasible in large datasets but the representative sampling of the imputations is still not as good as the classical approaches.

We recommend

- using multiple imputation where there is substantial missing data
- using `mice`, multiple imputation by chained equations, if computationally feasible
- considering random-forest if `mice` is not computationally feasible and missing-data bias is a major concern
- updating this document in a year or two, as the machine learning approaches are an active research area.



